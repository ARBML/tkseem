{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.FrequencyTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[536, 829]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([536, 829])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.SentencePieceTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3777, 1424, 78, 423, 9962, 560]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3777, 1424, 78, 423, 9962, 560])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading default vocab ...\n",
      "Reading the data ...\n",
      "Splitting the data ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.AutoTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3834, 8716, 4957]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1, 3834, 8716, 4957])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.RandomTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلا', '##م', 'علي', '##كم', 'أي', '##ها', 'ال', '##أصد', '##قاء']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use memory mapping to extract token's frequency for large files. It uses `mmap` to process chunks of the data at each iteration step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tokenizers as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "tokenizer = tk.FrequencyTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "\n",
    "# calculating time with memory mapping\n",
    "start_time = time.time()\n",
    "tokenizer.train(large_file = True)\n",
    "end_time = time.time()\n",
    "time_with_mmap = end_time - start_time\n",
    "\n",
    "# calculating time witout memory mapping\n",
    "start_time = time.time()\n",
    "tokenizer.train(large_file = False)\n",
    "end_time = time.time()\n",
    "time_without_mmap = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with memory mapping  0.3619396686553955\n",
      "Time without memory mapping  0.3943755626678467\n"
     ]
    }
   ],
   "source": [
    "print('Time with memory mapping ', time_with_mmap)\n",
    "print('Time without memory mapping ', time_without_mmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization vs Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tokenization to segment words using a pretrained dictionary. This makes segmentation very fast as compared to\n",
    "using libraries like `farasa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading default vocab ...\n",
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "9.40257453918457\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.AutoTokenizer()\n",
    "start_time = time.time()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "out =tokenizer.tokenize(open('data/raw/train.txt').read())\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Farasa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaid/.local/lib/python3.8/site-packages/farasa/__base.py:43: UserWarning: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Segmenting the data ...\n",
      "Splitting the data ...\n",
      "47.5738046169281\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.FrequencyTokenizer(segment = True)\n",
    "start_time = time.time()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved for deployment and reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Saving as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.FrequencyTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()\n",
    "tokenizer.save_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model without pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Loading as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.FrequencyTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.load_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_time(fun):\n",
    "    start_time = time.time()\n",
    "    fun().train()\n",
    "    return time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FrequencyTokenizer...\n",
      "Training SentencePiece...\n",
      "Training RandomTokenizer ...\n",
      "Training AutoTokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5d18e33eb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "import time \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "running_times = {}\n",
    "\n",
    "running_times['Frequency'] = calc_time(tk.FrequencyTokenizer)\n",
    "running_times['SentencePiece'] = calc_time(tk.SentencePieceTokenizer)\n",
    "running_times['Random'] = calc_time(tk.RandomTokenizer)\n",
    "running_times['Auto'] = calc_time(tk.AutoTokenizer)\n",
    "\n",
    "sns.barplot(data = pd.DataFrame.from_dict([running_times]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
