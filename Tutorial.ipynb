{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.FrequencyTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[536, 829]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([536, 829])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.SentencePieceTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3777, 1424, 78, 423, 9962, 560]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3777, 1424, 78, 423, 9962, 560])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading default vocab ...\n",
      "Reading the data ...\n",
      "Splitting the data ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.AutoTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3834, 8716, 4957]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([1, 3834, 8716, 4957])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.RandomTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلا', '##م', 'علي', '##كم', 'أي', '##ها', 'ال', '##أصد', '##قاء']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use memory mapping to extract token's frequency for large files. It uses `mmap` to process chunks of the data at each iteration step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tokenizers as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "tokenizer = tk.FrequencyTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "\n",
    "# calculating time with memory mapping\n",
    "start_time = time.time()\n",
    "tokenizer.train(large_file = True)\n",
    "end_time = time.time()\n",
    "time_with_mmap = end_time - start_time\n",
    "\n",
    "# calculating time witout memory mapping\n",
    "start_time = time.time()\n",
    "tokenizer.train(large_file = False)\n",
    "end_time = time.time()\n",
    "time_without_mmap = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time with memory mapping  0.3619396686553955\n",
      "Time without memory mapping  0.3943755626678467\n"
     ]
    }
   ],
   "source": [
    "print('Time with memory mapping ', time_with_mmap)\n",
    "print('Time without memory mapping ', time_without_mmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization vs Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tokenization to segment words using a pretrained dictionary. This makes segmentation very fast as compared to\n",
    "using libraries like `farasa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading default vocab ...\n",
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "9.40257453918457\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.AutoTokenizer()\n",
    "start_time = time.time()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "out =tokenizer.tokenize(open('data/raw/train.txt').read())\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Farasa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaid/.local/lib/python3.8/site-packages/farasa/__base.py:43: UserWarning: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Segmenting the data ...\n",
      "Splitting the data ...\n",
      "47.5738046169281\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.FrequencyTokenizer(segment = True)\n",
    "start_time = time.time()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved for deployment and reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Saving as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.FrequencyTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()\n",
    "tokenizer.save_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model without pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Loading as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.FrequencyTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.load_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_time(fun):\n",
    "    start_time = time.time()\n",
    "    fun().train()\n",
    "    return time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FrequencyTokenizer...\n",
      "Training SentencePiece...\n",
      "Training RandomTokenizer ...\n",
      "Training AutoTokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f59528934f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "import time \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "running_times = {}\n",
    "\n",
    "running_times['Frequency'] = calc_time(tk.FrequencyTokenizer)\n",
    "running_times['SentencePiece'] = calc_time(tk.SentencePieceTokenizer)\n",
    "running_times['Random'] = calc_time(tk.RandomTokenizer)\n",
    "running_times['Auto'] = calc_time(tk.AutoTokenizer)\n",
    "\n",
    "sns.barplot(data = pd.DataFrame.from_dict([running_times]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['h', '##e', '##l', '##l', '##o']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "base = tk.BaseTokenizer()\n",
    "base._split_word_cached(\"hello\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = {}\n",
    "for i in range(1, 20):\n",
    "    for j in range(1, 20):\n",
    "        if j <= i:\n",
    "            cached[i, j] = perumte(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(cached, open('cached.pl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0],\n",
       " [0, 0, 1, 0, 0],\n",
       " [0, 1, 0, 0, 0],\n",
       " [1, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perumte(5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
