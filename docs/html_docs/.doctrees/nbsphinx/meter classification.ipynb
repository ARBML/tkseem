{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poetry Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: pip: command not found\n",
      "/bin/bash: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install tkseem\n",
    "!pip install tnkeeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ARBML/tkseem/master/tasks/meter_classification/meters/data.txt\n",
    "!wget https://raw.githubusercontent.com/ARBML/tkseem/master/tasks/meter_classification/meters/labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tkseem as tk\n",
    "import tnkeeh as tn\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dense, Input, Dropout, Bidirectional, BatchNormalization, Flatten, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove diacritics\n",
      "Remove Tatweel\n",
      "Saving to meters/cleaned_data.txt\n",
      "Split data\n",
      "Save to data\n",
      "Read data  ['test_data.txt', 'test_lbls.txt', 'train_data.txt', 'train_lbls.txt']\n"
     ]
    }
   ],
   "source": [
    "tn.clean_data(file_path = 'meters/data.txt', save_path = 'meters/cleaned_data.txt', remove_diacritics=True, \n",
    "      execluded_chars=['!', '.', '?', '#'])\n",
    "tn.split_classification_data('meters/cleaned_data.txt', 'meters/labels.txt')\n",
    "train_data, test_data, train_lbls, test_lbls = tn.read_data(mode = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.CharacterTokenizer()\n",
    "tokenizer.train('data/train_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokenizer, data, labels):\n",
    "    X = tokenizer.encode_sentences(data)\n",
    "    y = np.array([int(lbl) for lbl in labels])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process training data\n",
    "X_train, y_train = preprocess(tokenizer, train_data, train_lbls)\n",
    "\n",
    "# process test data\n",
    "X_test, y_test = preprocess(tokenizer, test_data, test_lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(sent) for sent in X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input((max_length,)))\n",
    "model.add(Embedding(tokenizer.vocab_size, 256))\n",
    "model.add(Bidirectional(GRU(units = 256, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(units = 256, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(units = 256)))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(14, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "133/133 [==============================] - 465s 3s/step - loss: 2.3899 - accuracy: 0.1572 - val_loss: 1.9431 - val_accuracy: 0.2902\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 452s 3s/step - loss: 1.8384 - accuracy: 0.3214 - val_loss: 1.6722 - val_accuracy: 0.3905\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 436s 3s/step - loss: 1.5614 - accuracy: 0.4314 - val_loss: 1.5018 - val_accuracy: 0.4581\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 381s 3s/step - loss: 1.1860 - accuracy: 0.5879 - val_loss: 0.8718 - val_accuracy: 0.7109\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 370s 3s/step - loss: 0.7501 - accuracy: 0.7595 - val_loss: 0.5991 - val_accuracy: 0.8085\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 360s 3s/step - loss: 0.5233 - accuracy: 0.8410 - val_loss: 0.5352 - val_accuracy: 0.8332\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 361s 3s/step - loss: 0.4070 - accuracy: 0.8807 - val_loss: 0.4281 - val_accuracy: 0.8708\n",
      "Epoch 8/10\n",
      "133/133 [==============================] - 355s 3s/step - loss: 0.3229 - accuracy: 0.9074 - val_loss: 0.3947 - val_accuracy: 0.8841\n",
      "Epoch 9/10\n",
      "133/133 [==============================] - 356s 3s/step - loss: 0.2724 - accuracy: 0.9241 - val_loss: 0.3725 - val_accuracy: 0.8926\n",
      "Epoch 10/10\n",
      "133/133 [==============================] - 355s 3s/step - loss: 0.2301 - accuracy: 0.9352 - val_loss: 0.3540 - val_accuracy: 0.8989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff3a7692160>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split = 0.1, epochs = 10, batch_size= 256, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2name = ['السريع', 'الكامل', 'المتقارب', 'المتدارك', 'المنسرح', 'المديد', \n",
    "              'المجتث', 'الرمل', 'البسيط', 'الخفيف', 'الطويل', 'الوافر', 'الهزج', 'الرجز']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sentence):\n",
    "    sequence = tokenizer.encode_sentences([sentence], out_length = max_length)\n",
    "    pred = model.predict(sequence)[0]\n",
    "    print(label2name[np.argmax(pred, 0).astype('int')], np.max(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الرمل 0.9957462\n",
      "الكامل 0.98703927\n",
      "الكامل 0.9792284\n",
      "الطويل 0.99692947\n",
      "الهزج 0.94578993\n",
      "المديد 0.3755584\n",
      "الهزج 0.981885\n",
      "الرجز 0.8000305\n",
      "المتدارك 0.7176092\n",
      "المتدارك 0.99850094\n"
     ]
    }
   ],
   "source": [
    "classify(\"ما تردون على هذا المحب # دائبا يشكو إليكم في الكتب\")\n",
    "classify(\"ولد الهدى فالكائنات ضياء # وفم الزمان تبسم وسناء\")\n",
    "classify(\" لك يا منازل في القلوب منازل # أقفرت أنت وهن منك أواهل\")\n",
    "classify(\"ومن لم يمت بالسيف مات بغيره # تعددت الأسباب والموت واحد\")\n",
    "classify(\"أنا النبي لا كذب # أنا ابن عبد المطلب\")\n",
    "classify(\"هذه دراهم اقفرت # أم ربور محتها الدهور\")\n",
    "classify(\"هزجنا في بواديكم # فأجزلتم عطايانا\")\n",
    "classify(\"بحر سريع ماله ساحل # مستفعلن مستفعلن فاعلن\")\n",
    "classify(\"مَا مَضَى فَاتَ وَالْمُؤَمَّلُ غَيْبٌ # وَلَكَ السَّاعَةُ الَّتِيْ أَنْتَ فِيْهَا\")\n",
    "classify(\"يا ليلُ الصبّ متى غدهُ # أقيامُ الساعة موعدهُ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
