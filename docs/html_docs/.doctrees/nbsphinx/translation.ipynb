{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified version from https://www.tensorflow.org/tutorials/text/nmt_with_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranlsation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/home/zaid/.wget-hsts'. HSTS will be disabled.\n",
      "--2020-08-28 14:49:14--  https://raw.githubusercontent.com/ARBML/tkseem/master/tasks/translation/data/ar_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3705050 (3.5M) [text/plain]\n",
      "Saving to: ‘ar_data.txt’\n",
      "\n",
      "ar_data.txt         100%[===================>]   3.53M   719KB/s    in 5.1s    \n",
      "\n",
      "2020-08-28 14:49:21 (708 KB/s) - ‘ar_data.txt’ saved [3705050/3705050]\n",
      "\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/home/zaid/.wget-hsts'. HSTS will be disabled.\n",
      "--2020-08-28 14:49:21--  https://raw.githubusercontent.com/ARBML/tkseem/master/tasks/translation/data/en_data.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.112.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2510593 (2.4M) [text/plain]\n",
      "Saving to: ‘en_data.txt’\n",
      "\n",
      "en_data.txt         100%[===================>]   2.39M   588KB/s    in 4.2s    \n",
      "\n",
      "2020-08-28 14:49:26 (588 KB/s) - ‘en_data.txt’ saved [2510593/2510593]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ARBML/tkseem/master/tasks/translation/data/ar_data.txt\n",
    "!wget https://raw.githubusercontent.com/ARBML/tkseem/master/tasks/translation/data/en_data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tkseem\n",
    "!pip install tnkeeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import numpy as np\n",
    "import tkseem as tk\n",
    "import tnkeeh as tn\n",
    "import tensorflow as tf\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove diacritics\n",
      "Remove Tatweel\n",
      "Saving to ar_clean_data.txt\n",
      "Remove Tatweel\n",
      "Saving to en_clean_data.txt\n",
      "Split data\n",
      "Save to data\n",
      "Read data  ['ar_data.txt', 'en_data.txt', 'test_inp_data.txt', 'test_tar_data.txt', 'train_inp_data.txt', 'train_tar_data.txt']\n"
     ]
    }
   ],
   "source": [
    "tn.clean_data('ar_data.txt','ar_clean_data.txt', remove_diacritics=True)\n",
    "tn.clean_data('en_data.txt','en_clean_data.txt')\n",
    "\n",
    "tn.split_parallel_data('ar_clean_data.txt', 'en_clean_data.txt', split_ratio=0.3)\n",
    "train_inp_text, train_tar_text, test_inp_text, test_tar_text = tn.read_data(mode = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece ...\n",
      "Training SentencePiece ...\n"
     ]
    }
   ],
   "source": [
    "ar_tokenizer = tk.SentencePieceTokenizer(special_tokens=['<s>', '</s>'])\n",
    "ar_tokenizer.train('data/train_inp_data.txt')\n",
    "\n",
    "en_tokenizer = tk.SentencePieceTokenizer(special_tokens=['<s>', '</s>'])\n",
    "en_tokenizer.train('data/train_tar_data.txt')\n",
    "\n",
    "train_inp_data = ar_tokenizer.encode_sentences(train_inp_text, boundries = ('<s>', '</s>'))\n",
    "train_tar_data = en_tokenizer.encode_sentences(train_tar_text, boundries = ('<s>', '</s>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = len(train_inp_data)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_inp_data, train_tar_data)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "def get_loss_object():\n",
    "    return  tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 1))\n",
    "    loss_ = get_loss_object()(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 1024\n",
    "embedding_dim = 256\n",
    "max_length_inp = train_inp_data.shape[1]\n",
    "max_length_tar = train_tar_data.shape[1]\n",
    "steps_per_epoch = len(train_inp_data)//BATCH_SIZE\n",
    "vocab_inp_size = ar_tokenizer.vocab_size\n",
    "vocab_tar_size = en_tokenizer.vocab_size\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden, encoder, decoder, optimizer, en_tokenizer):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([en_tokenizer.token_to_id('<s>')] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "def train(epochs = 10, verbose = 0 ):\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "\n",
    "        for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden, encoder, decoder, optimizer, en_tokenizer)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            if batch % 100 == 0 and verbose:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                           batch,\n",
    "                                                           batch_loss.numpy()))\n",
    "\n",
    "        if verbose:\n",
    "            print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                              total_loss / steps_per_epoch))\n",
    "            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.7736\n",
      "Epoch 1 Batch 100 Loss 2.1184\n",
      "Epoch 1 Batch 200 Loss 1.7768\n",
      "Epoch 1 Batch 300 Loss 1.7248\n",
      "Epoch 1 Batch 400 Loss 1.6401\n",
      "Epoch 1 Loss 2.0055\n",
      "Time taken for 1 epoch 1444.5116345882416 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.6100\n",
      "Epoch 2 Batch 100 Loss 1.5598\n",
      "Epoch 2 Batch 200 Loss 1.5922\n",
      "Epoch 2 Batch 300 Loss 1.5228\n",
      "Epoch 2 Batch 400 Loss 1.4033\n",
      "Epoch 2 Loss 1.5530\n",
      "Time taken for 1 epoch 1424.0314059257507 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.2111\n",
      "Epoch 3 Batch 100 Loss 1.4820\n",
      "Epoch 3 Batch 200 Loss 1.3912\n",
      "Epoch 3 Batch 300 Loss 1.4882\n",
      "Epoch 3 Batch 400 Loss 1.2942\n",
      "Epoch 3 Loss 1.3888\n",
      "Time taken for 1 epoch 1441.213187456131 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 1.2663\n",
      "Epoch 4 Batch 100 Loss 1.3889\n",
      "Epoch 4 Batch 200 Loss 1.1667\n",
      "Epoch 4 Batch 300 Loss 1.2853\n",
      "Epoch 4 Batch 400 Loss 1.2746\n",
      "Epoch 4 Loss 1.2559\n",
      "Time taken for 1 epoch 1422.2563009262085 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.1258\n",
      "Epoch 5 Batch 100 Loss 1.1021\n",
      "Epoch 5 Batch 200 Loss 1.1365\n",
      "Epoch 5 Batch 300 Loss 1.1450\n",
      "Epoch 5 Batch 400 Loss 1.3664\n",
      "Epoch 5 Loss 1.1176\n",
      "Time taken for 1 epoch 1378.149689912796 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.9396\n",
      "Epoch 6 Batch 100 Loss 1.0216\n",
      "Epoch 6 Batch 200 Loss 1.1066\n",
      "Epoch 6 Batch 300 Loss 1.0084\n",
      "Epoch 6 Batch 400 Loss 1.1767\n",
      "Epoch 6 Loss 0.9732\n",
      "Time taken for 1 epoch 1328.8411734104156 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.9608\n",
      "Epoch 7 Batch 100 Loss 0.8912\n",
      "Epoch 7 Batch 200 Loss 0.8274\n",
      "Epoch 7 Batch 300 Loss 0.8302\n",
      "Epoch 7 Batch 400 Loss 0.7896\n",
      "Epoch 7 Loss 0.8303\n",
      "Time taken for 1 epoch 1294.177453994751 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.6882\n",
      "Epoch 8 Batch 100 Loss 0.6465\n",
      "Epoch 8 Batch 200 Loss 0.7108\n",
      "Epoch 8 Batch 300 Loss 0.7176\n",
      "Epoch 8 Batch 400 Loss 0.7323\n",
      "Epoch 8 Loss 0.7000\n",
      "Time taken for 1 epoch 1367.661788702011 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.5313\n",
      "Epoch 9 Batch 100 Loss 0.4794\n",
      "Epoch 9 Batch 200 Loss 0.6126\n",
      "Epoch 9 Batch 300 Loss 0.6033\n",
      "Epoch 9 Batch 400 Loss 0.5891\n",
      "Epoch 9 Loss 0.5853\n",
      "Time taken for 1 epoch 1372.0978388786316 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.5009\n",
      "Epoch 10 Batch 100 Loss 0.5200\n",
      "Epoch 10 Batch 200 Loss 0.4687\n",
      "Epoch 10 Batch 300 Loss 0.4556\n",
      "Epoch 10 Batch 400 Loss 0.4321\n",
      "Epoch 10 Loss 0.4802\n",
      "Time taken for 1 epoch 1334.807544708252 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_tar, max_length_inp))\n",
    "\n",
    "    inputs = ar_tokenizer.encode_sentences([sentence], boundries = ('<s>', '</s>'), \n",
    "                                  out_length = max_length_inp)\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([en_tokenizer.token_to_id('<s>')], 0)\n",
    "\n",
    "    for t in range(max_length_tar):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += en_tokenizer.id_to_token(predicted_id) + ' '\n",
    "\n",
    "        if en_tokenizer.id_to_token(predicted_id) == '</s>':\n",
    "            return result, sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence\n",
    "\n",
    "def translate(sentences, translations, verbose = 1):\n",
    "    inputs = sentences\n",
    "    outputs = []\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        result, sentence = evaluate(sentence)\n",
    "        result = ar_tokenizer.detokenize(result)\n",
    "        result = result.replace('<s>', '').replace('</s>', '')\n",
    "        result = re.sub(' +', ' ', result)\n",
    "        outputs.append(result)\n",
    "        if verbose:\n",
    "            print('inpt: %s' % (sentence))\n",
    "            print('pred: {}'.format(result))\n",
    "            print('true: {}'.format(translations[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpt:  حسنا هناك بنك لك\n",
      "pred:  Well there ' s the name for you \n",
      "true: Well there ' s a bank for you \n",
      "inpt:  ماذا حدث يا أبي\n",
      "pred:  What happened Dad \n",
      "true: What happened Father \n",
      "inpt:  حسنا لقد مرت أربع سنوات تقريبا\n",
      "pred:  Well I ' ll be years since \n",
      "true: Well it ' s almost four years now \n",
      "inpt:  هذا صحيح أليس كذلك ما\n",
      "pred:  That ' s right isn ' t it \n",
      "true: That ' s right ain ' t it Ma \n",
      "inpt:  أربع سنوات أربع سنوات 5 يونيو بنسلفانيا\n",
      "pred:  Four months years of the floor \n",
      "true: Four years Four years 5th June Pa \n",
      "inpt:  لم أستطع مواكبة المدفوعات\n",
      "pred:  I couldn ' t steal up for the prisoner ' s jewels \n",
      "true: I couldn ' t keep up the payments \n",
      "inpt:  تتذكره\n",
      "pred:  You remember him \n",
      "true: You remember him \n",
      "inpt:  راندي دنلاب\n",
      "pred:  The Potem oin the toxic ms \n",
      "true: Randy Dunlap \n",
      "inpt:  لحاء الشجر\n",
      "pred:  l journey less \n",
      "true: Bark \n",
      "inpt:  لذلك دخلت وطلب مني أن أجلس\n",
      "pred:  So he died and keep me to stop \n",
      "true: So I dropped in and he asked me to sit down \n",
      "inpt:  جورج هل تعرف ماذا كان يرتدي\n",
      "pred:  George do you know what was \n",
      "true: George do you know what he was wearing \n",
      "inpt:  كيمونو\n",
      "pred:  Kim was \n",
      "true: A kimono \n",
      "inpt:  لا\n",
      "pred:  No \n",
      "true: No \n",
      "inpt:  بلى\n",
      "pred:  Yeah \n",
      "true: Yeah \n",
      "inpt:  أوه الآن اللحاء\n",
      "pred:  Oh now ' s silly \n",
      "true: Oh now Bark \n",
      "inpt:  يجب أن يكون ثوب خلع الملابس\n",
      "pred:  The ve got a big room \n",
      "true: It must have been a dressing gown \n",
      "inpt:  أنا أعرف ثوب خلع الملابس عندما أراه\n",
      "pred:  I know the whole bedroom \n",
      "true: I know a dressing gown when I see it \n",
      "inpt:  كان كيمونو جورج\n",
      "pred:  He was amazing mom \n",
      "true: It was a kimono George \n",
      "inpt:  هل لفساتين الملابس الزهور على م\n",
      "pred:  Are you a rooster with the prisoner glasses \n",
      "true: Do dressing gowns have flowers on ' em \n",
      "inpt:  أوه اللحاء\n",
      "pred:  Oh Merna \n",
      "true: Oh Bark \n",
      "inpt:  لا مانع من ذلك يا أبي\n",
      "pred:  Don ' t mind who is my father \n",
      "true: Never mind that Father \n",
      "inpt:  ماذا قال\n",
      "pred:  What did he say \n",
      "true: What did he say \n",
      "inpt:  أوه لقد كان لطيفا بما فيه الكفاية\n",
      "pred:  Oh he ' s my mom enough \n",
      "true: Oh he was nice enough \n",
      "inpt:  أوه الآن اللحاء\n",
      "pred:  Oh now ' s silly \n",
      "true: Oh now Bark \n",
      "inpt:  نعم لقد فعل\n",
      "pred:  Yes he ' s done \n",
      "true: Yeah he did \n",
      "inpt:  كم من الوقت أعطاك يا أبي\n",
      "pred:  How long time is still Dad \n",
      "true: How much time did he give you Father \n",
      "inpt:  ستة أشهر\n",
      "pred:  Nine months \n",
      "true: Six months \n",
      "inpt:  يا حسنا إذن لا يوجد اندفاع فوري\n",
      "pred:  Oh Well uh we ' s no coffin \n",
      "true: Oh Oh well then there ' s no immediate rush \n",
      "inpt:  متى تصل الشهور الستة\n",
      "pred:  When did you a pot s I inquire \n",
      "true: When are the six months up \n",
      "inpt:  الثلاثاء\n",
      "pred:  Paper \n",
      "true: Tuesday \n",
      "inpt:  لكن ولكن لماذا لم تخبرنا عاجلا\n",
      "pred:  But but why didn ' t you stop them \n",
      "true: But but why didn ' t you tell us sooner \n",
      "inpt:  الثلاثاء\n",
      "pred:  Paper \n",
      "true: Tuesday \n",
      "inpt:  لا يعطينا الكثير من الوقت أليس كذلك\n",
      "pred:  Don ' t give them a lot of time is it \n",
      "true: Doesn ' t give us much time does it \n",
      "inpt:  أو\n",
      "pred:  Or \n",
      "true: Or \n",
      "inpt:  هذا صحيح\n",
      "pred:  That ' s right \n",
      "true: That ' s right \n",
      "inpt:  بالطبع\n",
      "pred:  Of course \n",
      "true: Oh sure \n",
      "inpt:  الذي أعطاك هذا اللباس جيش الخلاص\n",
      "pred:  The d put this place is a few weeks \n",
      "true: Who gave you that dress the Salvation Army \n",
      "inpt:  و\n",
      "pred:  And \n",
      "true: And uh \n",
      "inpt:  بلى\n",
      "pred:  Yeah \n",
      "true: Yeah \n",
      "inpt:  حسنا لا أستطيع فعل ذلك بمفردي\n",
      "pred:  Well I can ' t do it on your own \n",
      "true: Well I can ' t do it alone \n",
      "inpt:  لا إنها لم ترسل لنا البرتقالي\n",
      "pred:  No she ' s not your delicate ed and the Israelites \n",
      "true: No she ' s never even sent us an orange \n",
      "inpt:  نعم ولكن ماذا عن هارفي\n",
      "pred:  Yes but what brings about \n",
      "true: Yes but what about Harvey \n",
      "inpt:  أوه نحن لا نريد أن نسأل هارفي\n",
      "pred:  Oh we don ' t want to remember my mom \n",
      "true: Oh we wouldn ' t want to ask Harvey \n",
      "inpt:  أوه لا لن نسأل هارفي\n",
      "pred:  Oh no I wouldn ' t die \n",
      "true: Oh no we wouldn ' t ask Harvey \n",
      "inpt:  لا طلبنا من هارفي الزواج من نيلي\n",
      "pred:  Don ' t you caught my mom did you Rachel \n",
      "true: No we asked Harvey to marry Nellie \n",
      "inpt:  لا يمكننا أن نتوقع من الرجل أن يفعل أكثر من ذلك\n",
      "pred:  We can ' t we ' ve got a man can do that \n",
      "true: We can ' t expect the guy to do more than that \n",
      "inpt:  روبرت توقف عن الحديث بهذه الطريقة\n",
      "pred:  Elizabeth stop talking to the way \n",
      "true: Robert stop talking that way \n",
      "inpt:  قصها يا روبرت\n",
      "pred:  A spear it Robert \n",
      "true: Cut it out Robert \n",
      "inpt:  ليس لدي مجال لكلا منكما\n",
      "pred:  I haven ' t the whole world I ' re in \n",
      "true: I haven ' t room for both of you \n",
      "inpt:  لا يوجد سوى أريكة صغيرة في غرفة المعيشة\n",
      "pred:  There ' s nothing for a big tree in the street \n",
      "true: There ' s only a small couch in the living room \n"
     ]
    }
   ],
   "source": [
    "translate(test_inp_text[:50], test_tar_text[:50], verbose = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
