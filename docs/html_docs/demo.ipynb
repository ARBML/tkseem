{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkseem as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training WordTokenizer...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[557, 798]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"السلام عليكم\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['السلام', 'عليكم']\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "السلام عليكم\n"
     ]
    }
   ],
   "source": [
    "detokenized = tokenizer.detokenize(decoded)\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training SentencePiece...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.SentencePieceTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1799, 2741]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"السلام عليكم\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁السلام', '▁عليكم']\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " السلام عليكم\n"
     ]
    }
   ],
   "source": [
    "detokenized = tokenizer.detokenize(decoded)\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training AutoTokenizer...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.AutoTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 367, 764, 184]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"السلام عليكم\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ال', '##سلام', 'علي', '##كم']\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training RandomTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.RandomTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلا', '##م', 'علي', '##كم', 'أ', '##يها', 'ال', '##أصد', '##قاء']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disjoint Letter Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training DisjointLetterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.DisjointLetterTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ا', '##لسلا', '##م', 'عليكم', 'أ', '##يها', 'ا', '##لأ', '##صد', '##قا', '##ء']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.CharacterTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ا', '##ل', '##س', '##ل', '##ا', '##م', 'ع', '##ل', '##ي', '##ك', '##م']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use memory mapping to extract token's frequency for large files. It uses `mmap` to process chunks of the data at each iteration step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training WordTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "\n",
    "# training \n",
    "tokenizer.train(large_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization vs Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tokenization to segment words using a pretrained dictionary. This makes segmentation very fast as compared to\n",
    "using libraries like `farasa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training AutoTokenizer...\n",
      "37.64352488517761\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tokenizer = tk.AutoTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "tokenizer.train()\n",
    "\n",
    "start_time = time.time()\n",
    "out = tokenizer.tokenize(open('data/raw/train.txt').read(), cache = True)\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Farasa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaid/.local/lib/python3.8/site-packages/farasa/__base.py:43: UserWarning: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Segmenting the data ...\n",
      "50.82935118675232\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer(segment = True)\n",
    "start_time = time.time()\n",
    "tokenizer.process_data('data.txt')\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved for deployment and reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Training WordTokenizer...\n",
      "Saving as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('data.txt')\n",
    "tokenizer.train()\n",
    "tokenizer.save_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model without pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.load_model('freq.pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('السلام عليكم')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer...\n",
      "Training SentencePiece...\n",
      "Training RandomTokenizer ...\n",
      "Training AutoTokenizer...\n",
      "Training DisjointLetterTokenizer ...\n",
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def calc_time(fun):\n",
    "    start_time = time.time()\n",
    "    fun().train()\n",
    "    return time.time() - start_time\n",
    "\n",
    "running_times = {}\n",
    "\n",
    "running_times['Word'] = calc_time(tk.WordTokenizer)\n",
    "running_times['SP'] = calc_time(tk.SentencePieceTokenizer)\n",
    "running_times['Random'] = calc_time(tk.RandomTokenizer)\n",
    "running_times['Auto'] = calc_time(tk.AutoTokenizer)\n",
    "running_times['Disjoint'] = calc_time(tk.DisjointLetterTokenizer)\n",
    "running_times['Character'] = calc_time(tk.CharacterTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7efff11ede50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(data = pd.DataFrame.from_dict([running_times]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
