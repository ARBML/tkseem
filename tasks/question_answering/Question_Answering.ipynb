{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Question Answering.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLKQCb3q4wtA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tkseem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYTxg7qC4ws9",
        "colab_type": "text"
      },
      "source": [
        "## Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsn9KrrTwCYV",
        "colab_type": "text"
      },
      "source": [
        "### Download and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1LjtNnF4ws9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/MLQA/MLQA_V1.zip\n",
        "!unzip MLQA_V1.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56vO4ShhwQXJ",
        "colab_type": "text"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0n4Kkku5SRg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json \n",
        "\n",
        "def read_data(file_path, max_context_size = 100):\n",
        "  # Read dataset\n",
        "  with open(file_path) as f:\n",
        "      data = json.load(f)\n",
        "\n",
        "  contexts = []\n",
        "  questions = []\n",
        "  answers = []\n",
        "  labels = []\n",
        "  \n",
        "  for i in range(len(data['data'])):\n",
        "\n",
        "    paragraph_object = data['data'][i][\"paragraphs\"]\n",
        "    \n",
        "    for j in range(len(paragraph_object)):\n",
        "\n",
        "      context_object = paragraph_object[j]\n",
        "      context_text = context_object['context']\n",
        "\n",
        "      if len(context_text.split()) > max_context_size:\n",
        "        continue\n",
        "      for k in range(len(context_object['qas'])):\n",
        "\n",
        "        question_object = context_object['qas'][k]\n",
        "        question_text = question_object['question']\n",
        "        \n",
        "        answer_object = question_object['answers'][0]\n",
        "        answer_text = answer_object['text']\n",
        "        answer_start = answer_object['answer_start']\n",
        "        answer_end = answer_start + len(answer_text)\n",
        "\n",
        "        answer_start = len(context_text[:answer_start].split())\n",
        "        answer_end = answer_start + len(answer_text.split())\n",
        "        if answer_end >= max_context_size:\n",
        "          answer_end = max_context_size -1\n",
        "        labels.append([answer_start, answer_end])\n",
        "        questions.append(question_text)\n",
        "        contexts.append(context_text)\n",
        "        answers.append(answer_text)\n",
        "  \n",
        "  with open('train_contexts.txt', 'w') as f:\n",
        "    f.write(('\\n').join(contexts))\n",
        "  with open('train_questions.txt', 'w') as f:\n",
        "    f.write(('\\n').join(questions))\n",
        "  return {'qas':questions, 'ctx':contexts, 'ans':answers, 'lbl':labels}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVxr4zyBrK45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = read_data('MLQA_V1/test/test-context-ar-question-ar.json')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjya3QAVZJf7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "outputId": "9c014fce-63fd-407b-f2c3-75d057acea08"
      },
      "source": [
        "for i in range(10):\n",
        "  print(train_data['qas'][i])\n",
        "  print(train_data['ctx'][i])\n",
        "  print(train_data['ans'][i])\n",
        "  print(\"==============\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ما الذي جعل شريط الاختبار للطائرة؟\n",
            "بحيرة جرووم كانت تستخدم للقصف المدفعي والتدريب علي المدفعية خلال الحرب العالمية الثانية، ولكن تم التخلي عنها بعد ذلك حتى نيسان / أبريل 1955، عندما تم اختياره من قبل فريق لوكهيد اسكنك كموقع مثالي لاختبار لوكهيد يو-2 - 2 طائرة التجسس. قاع البحيرة قدم الشريط المثالية التي يمكن عمل اختبارات الطائرات المزعجة، ارتفاع سلسلة جبال وادي الإيمجرانت ومحيط NTS يحمي موقع الاختبار من أعين المتطفلين والتدخل الخارجي.\n",
            "قاع البحيرة\n",
            "==============\n",
            "من كان يرافق طائرة يو -2 عند التسليم؟\n",
            "شيدت لوكهيد قاعدة مؤقتة في الموقع، ثم عرفت باسم الموقع الثاني أو \"المزرعة\"، التي تتألف من أكثر بقليل من بضعة مخابئ، وحلقات عمل ومنازل متنقلة لفريقها الصغير. في ثلاثة أشهر فقط شيد مدرج طوله 5000  ودخل الخدمة بحلول تموز / يوليو 1955. حصلت المزرعة على تسليم أول يو 2 في 24 يوليو، 1955 من بوربانك على سي 124 جلوب ماستر الثاني طائرة شحن، يرافقه فنيي وكهيد على دي سي 3. انطلق أول يو - 2 من الجرووم في 4 أغسطس، 1955. بدأت عمليات تحليق أسطول يو 2 تحت سيطرة وكالة المخابرات المركزية الأمريكية في الأجواء السوفياتية بحلول منتصف عام 1956.\n",
            "فنيي وكهيد\n",
            "==============\n",
            " ما هو نوع العمل الذي يواجهه الطيارون العسكريون إذا انتقلوا إلى \\n مناطق محظورة؟ \n",
            "على عكس الكثير من حدود نيليس، والمنطقة المحيطة بها في البحيرة بشكل دائم خارج الحدود سواء على المدنيين وطبيعية حركة الطيران العسكري. محطات الرادار لحماية المنطقة، والأفراد غير مصرح بها سرعان ما تطرد. حتى طيارين التدريب العسكري في خطر NAFR إجراءات التأديبية إذا تواجدوا بطريق الخطأ في \"المربع\"الحظور للجرووم والأجواء المحيطة بها.\n",
            "إجراءات التأديبية\n",
            "==============\n",
            "متى تم نشر مقال مجلة الطيران؟\n",
            "في كانون الثاني 2006، نشر مؤرخ الفضاء دواين أ يوم مقال نشر في المجلة الإلكترونية الطيران والفضاء استعراض بعنوان \"رواد الفضاء والمنطقة 51 : حادث سكايلاب\". المقال كان مبنيا على مذكرة مكتوبة في عام 1974 إلى مديروكالة المخابرات المركزية يام كولبي من قبل عملاء مجهولين لوكالة الاستخبارات المركزية. وذكرت المذكرة أن رواد الفضاء على متن سكايلاب 4، وذلك كجزء من برنامج أوسع نطاقا، عن غير قصد بالتقاط صور لموقع الذي قالت المذكرة :\n",
            "كانون الثاني 2006\n",
            "==============\n",
            "ما هو الموقع الذي أصبح مركزاً للأطباق الطائرة ونظريات المؤامرة؟\n",
            "لطبيعتها السرية وفيما لا شك فيه بحوث تصنيف الطائرات، إلى جانب تقارير عن الظواهر غير العادية، قد أدت الي ان تصبح منطقة 51 مركزا للاطباق الطائرة الحديثة ونظريات المؤامرة. بعض الأنشطة المذكورة في مثل هذه النظريات في منطقة 51 تشمل ما يلي :\n",
            "منطقة 51\n",
            "==============\n",
            "ما كان محور مؤامرة الجسم الغريب الحديثة؟\\n\n",
            "لطبيعتها السرية وفيما لا شك فيه بحوث تصنيف الطائرات، إلى جانب تقارير عن الظواهر غير العادية، قد أدت الي ان تصبح منطقة 51 مركزا للاطباق الطائرة الحديثة ونظريات المؤامرة. بعض الأنشطة المذكورة في مثل هذه النظريات في منطقة 51 تشمل ما يلي :\n",
            "منطقة 51\n",
            "==============\n",
            "مالذي يُظن بأنه قد تم بنائه في روزويل؟\n",
            "التخزين، والفحص، والهندسة العكسية للمركبة الفضائية الغريبة المحطمة (بما في ذلك مواد يفترض ان تعافى في روزويل)، ودراسة شاغليها (حية أو ميتة)، وصناعة الطائرات على أساس التكنولوجيا الغريبة.\n",
            "صناعة الطائرات على أساس التكنولوجيا الغريبة\n",
            "==============\n",
            "متى يقوم Qos بالتفاوض على كيفية عمل الشبكة؟\n",
            "ويمكن أن تتوافق الشبكة أو البروتوكول الذي يدعم جودة الخدمات على عقد المرور مع تطبيق البرمجيات والقدرة الاحتياطية في عقد الشبكة، على سبيل المثال خلال مرحلة إقامة الدورات. وهي يمكن أن تحقق رصدا لمستوى الأداء خلال الدورة، على سبيل المثال معدل البيانات والتأخير، والتحكم ديناميكيا عن طريق جدولة الأولويات في عقد الشبكة. وقد تفرج عن القدرة الاحتياطية خلال مرحلة الهدم.\n",
            "مرحلة إقامة الدورات\n",
            "==============\n",
            "ما هو أحد الشروط للتجارة الشبكية المتنوعة؟\n",
            "جودة الخدمة قد تكون مطلوبة لأنواع معينة من حركة مرور الشبكة، على سبيل المثال :\n",
            "جودة الخدمة\n",
            "==============\n",
            "كم عدد قوائم الانتظار الموجودة على أجهزة توجيه المختلفة؟\\n\n",
            "الموجهات لدعم DiffServ استخدام قوائم متعددة للحزم في انتظار انتقال من عرض النطاق الترددي مقيدة (على سبيل المثال، منطقة واسعة) واجهات. راوتر الباعة يوفر قدرات مختلفة لتكوين هذا السلوك، لتشمل عددا من قوائم معتمدة، والأولويات النسبية لقوائم الانتظار، وعرض النطاق الترددي المخصصة لكل قائمة انتظار.\n",
            "متعددة\n",
            "==============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfA0kQYBwdGl",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrFV7p7f4wtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "import time\n",
        "import numpy as np\n",
        "import tkseem as tk\n",
        "import tensorflow as tf\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsKGssPu4wtH",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJyd7eXI4wtI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "bd8323f0-d8aa-4a59-e266-d1a74349bfa1"
      },
      "source": [
        "qa_tokenizer = tk.WordTokenizer()\n",
        "qa_tokenizer.train('train_questions.txt')\n",
        "print('Vocab size ', qa_tokenizer.vocab_size)\n",
        "\n",
        "cx_tokenizer = tk.WordTokenizer()\n",
        "cx_tokenizer.train('train_contexts.txt')\n",
        "print('Vocab size ', cx_tokenizer.vocab_size)\n",
        "\n",
        "train_inp_data = qa_tokenizer.encode_sentences(train_data['qas'])\n",
        "train_tar_data = cx_tokenizer.encode_sentences(train_data['ctx'])\n",
        "train_tar_lbls = train_data['lbl']\n",
        "train_inp_data.shape, train_tar_data.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training WordTokenizer ...\n",
            "Vocab size  8883\n",
            "Training WordTokenizer ...\n",
            "Vocab size  10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2936, 23), (2936, 100))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB9izdqz4wtK",
        "colab_type": "text"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTPgEJ_o4wtL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(train_inp_data)\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_inp_data, train_tar_data, train_tar_lbls)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPIRHUAmw5qH",
        "colab_type": "text"
      },
      "source": [
        "### Create Encoder and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28dhyDseWdGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output = self.gru(x, initial_state = hidden)\n",
        "        return output\n",
        "\n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))\n",
        "  \n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, output_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.output_sz = output_sz\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                       return_sequences=False,\n",
        "                                       recurrent_initializer='glorot_uniform')\n",
        "        self.fc11 = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.fc12 = tf.keras.layers.Dense(output_sz)\n",
        "\n",
        "        self.fc21 = tf.keras.layers.Dense(embedding_dim)\n",
        "        self.fc22 = tf.keras.layers.Dense(output_sz)\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = self.gru(x, initial_state = hidden)\n",
        "        x1 = self.fc11(x)\n",
        "        x2 = self.fc21(x)\n",
        "\n",
        "        x1 = self.fc12(x1)\n",
        "        x2 = self.fc22(x2)\n",
        "        return [x1, x2]\n",
        "\n",
        "def loss_fn(true, pred):\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  return (cross_entropy(true[:,0:1], pred[0]) + cross_entropy(true[:,1:2], pred[1]))/2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPlRQTWix3sd",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruIcHwa1W2VF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "units = 1024\n",
        "embedding_dim = 256\n",
        "max_length_inp = train_inp_data.shape[1]\n",
        "max_length_tar = train_tar_data.shape[1]\n",
        "vocab_tar_size = cx_tokenizer.vocab_size\n",
        "vocab_inp_size = qa_tokenizer.vocab_size\n",
        "steps_per_epoch = len(train_inp_data) // BATCH_SIZE\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, max_length_tar)\n",
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "optim = tf.optimizers.Adam()"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6R5--spXGJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "e0d89f20-77c3-43a5-d0b1-b211fc5b0130"
      },
      "source": [
        "epochs = 25\n",
        "for epoch in range(epochs):\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  epoch_loss = 0\n",
        "  \n",
        "  for idx, (inp, tar, true) in enumerate(dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "        hidden = encoder(inp, enc_hidden)\n",
        "        pred = decoder(tar, hidden)\n",
        "        loss  = loss_fn(true, pred)\n",
        "    variables = decoder.trainable_variables + encoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optim.apply_gradients(zip(gradients, variables))\n",
        "    epoch_loss += loss.numpy()\n",
        "  print(f\"Epoch {epoch} loss: {epoch_loss/steps_per_epoch:.3f}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 loss: 4.386\n",
            "Epoch 1 loss: 4.264\n",
            "Epoch 2 loss: 4.238\n",
            "Epoch 3 loss: 4.105\n",
            "Epoch 4 loss: 3.932\n",
            "Epoch 5 loss: 3.758\n",
            "Epoch 6 loss: 3.643\n",
            "Epoch 7 loss: 3.548\n",
            "Epoch 8 loss: 3.456\n",
            "Epoch 9 loss: 3.382\n",
            "Epoch 10 loss: 3.285\n",
            "Epoch 11 loss: 3.215\n",
            "Epoch 12 loss: 3.141\n",
            "Epoch 13 loss: 3.047\n",
            "Epoch 14 loss: 2.916\n",
            "Epoch 15 loss: 2.831\n",
            "Epoch 16 loss: 2.748\n",
            "Epoch 17 loss: 2.614\n",
            "Epoch 18 loss: 2.462\n",
            "Epoch 19 loss: 2.306\n",
            "Epoch 20 loss: 2.126\n",
            "Epoch 21 loss: 1.944\n",
            "Epoch 22 loss: 1.770\n",
            "Epoch 23 loss: 1.637\n",
            "Epoch 24 loss: 1.414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhHVvsnO4wtW",
        "colab_type": "text"
      },
      "source": [
        "### evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKBQLURa4wtW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def answer(question_txt, context_txt, answer_txt_tru):\n",
        "    question = qa_tokenizer.encode_sentences([question_txt], out_length = max_length_inp)\n",
        "    context = cx_tokenizer.encode_sentences([context_txt], out_length = max_length_tar)\n",
        "    question = tf.convert_to_tensor(question)\n",
        "    context = tf.convert_to_tensor(context)\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_hidden = encoder(question, hidden)\n",
        "    pred = decoder(context, enc_hidden)\n",
        "\n",
        "    start = tf.argmax(pred[0], axis = -1).numpy()[0]\n",
        "    end = tf.argmax(pred[1], axis = -1).numpy()[0]\n",
        "    \n",
        "    if start >= len(context_txt.split()):\n",
        "      start = len(context_txt.split()) - 1\n",
        "    if end >= len(context_txt.split()):\n",
        "      end = len(context_txt.split()) - 1\n",
        "    \n",
        "    # if one word prediction\n",
        "    if end == start:\n",
        "      end += 1\n",
        "    answer_txt = (' ').join(context_txt.split()[start:end])\n",
        "    \n",
        "    print(\"Question : \", question_txt)\n",
        "    print(\"Context  : \",context_txt)\n",
        "    print(\"Pred Answer : \",answer_txt)\n",
        "    print(\"True Answer : \", answer_txt_tru)\n",
        "    print(\"======================\")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1Kazqer_JFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "98936ff0-67b1-4198-f67c-9992ccd9d798"
      },
      "source": [
        "answer(\"في أي عام توفي وليام ؟\", \"توفي وليام في عام 1990\", \"1990\")\n",
        "answer(\"ماهي عاصمة البحرين ؟\", \"عاصمة البحرين هي المنامة\", \"المنامة\")\n",
        "answer(\"في أي دولة ولد جون ؟\", \"ولد في فرنسا عام 1988\", \"فرنسا\")\n",
        "answer(\"أين تركت الهاتف ؟\", \"تركت الهاتف فوق الطاولة\", \"فوق الطاولة\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question :  في أي عام توفي وليام ؟\n",
            "Context  :  توفي وليام في عام 1990\n",
            "Pred Answer :  1990\n",
            "True Answer :  1990\n",
            "======================\n",
            "Question :  ماهي عاصمة البحرين ؟\n",
            "Context  :  عاصمة البحرين هي المنامة\n",
            "Pred Answer :  المنامة\n",
            "True Answer :  المنامة\n",
            "======================\n",
            "Question :  في أي دولة ولد جون ؟\n",
            "Context  :  ولد في فرنسا عام 1988\n",
            "Pred Answer :  1988\n",
            "True Answer :  فرنسا\n",
            "======================\n",
            "Question :  أين تركت الهاتف ؟\n",
            "Context  :  تركت الهاتف فوق الطاولة\n",
            "Pred Answer :  الطاولة\n",
            "True Answer :  فوق الطاولة\n",
            "======================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}