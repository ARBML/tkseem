{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "import sys\n",
    "# sys.path.append('..')\n",
    "import tkseem as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'حالكم'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphBert(tk.MorphologicalTokenizer):\n",
    "    \n",
    "    max_input_chars_per_word = 10\n",
    "    \n",
    "    def _tokenize_from_dict(self):\n",
    "        pass\n",
    "    \n",
    "    def convert_to_unicode(self,text):\n",
    "        \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "        if six.PY3:\n",
    "            if isinstance(text, str):\n",
    "                return text\n",
    "            elif isinstance(text, bytes):\n",
    "                return text.decode(\"utf-8\", \"ignore\")\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "        elif six.PY2:\n",
    "            if isinstance(text, str):\n",
    "                return text.decode(\"utf-8\", \"ignore\")\n",
    "            elif isinstance(text, unicode):\n",
    "                return text\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "        else:\n",
    "            raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "    def whitespace_tokenize(self, text):\n",
    "        \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "        text = text.strip()\n",
    "        if not text:\n",
    "            return []\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "    \n",
    "    def tokenize(self,word):\n",
    "        return self._split_word(word)\n",
    "        \n",
    "    def _split_word(self, text):\n",
    "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
    "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
    "        using the given vocabulary.\n",
    "        For example:\n",
    "          input = \"unaffable\"\n",
    "          output = [\"un\", \"##aff\", \"##able\"]\n",
    "        Args:\n",
    "          text: A single token or whitespace separated tokens. This should have\n",
    "            already been passed through `BasicTokenizer.\n",
    "        Returns:\n",
    "          A list of wordpiece tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        text = self.convert_to_unicode(text)\n",
    "\n",
    "        output_tokens = []\n",
    "        for token in self.whitespace_tokenize(text):\n",
    "            chars = list(token)\n",
    "            if len(chars) > self.max_input_chars_per_word:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "\n",
    "            is_bad = False\n",
    "            start = 0\n",
    "            sub_tokens = []\n",
    "            while start < len(chars):\n",
    "                end = len(chars)\n",
    "                cur_substr = None\n",
    "                while start < end:\n",
    "                    substr = \"\".join(chars[start:end])\n",
    "                    if start > 0:\n",
    "                        substr = \"##\" + substr\n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                    end -= 1\n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorphGenerators(tk.MorphologicalTokenizer):\n",
    "    \n",
    "    def _tokenize_from_dict(self, text, freq_dict, cache=False, max_size=20):\n",
    "        \"\"\"Tokenize using frequency based approach given a dictionary\n",
    "\n",
    "        Args:\n",
    "            text (str): input string\n",
    "            freq_dict (dict): frequency dictionary\n",
    "            cache (bool, optional): faster approach. Defaults to False.\n",
    "            max_size (int, optional): maximum word size. Defaults to 20.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        assert freq_dict\n",
    "        tokens = []\n",
    "        output_tokens = []\n",
    "        for word in text.split():\n",
    "            if len(word) >= max_size:\n",
    "                print(f\"{word} is too long ...\")\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "            if word in freq_dict:\n",
    "                output_tokens.append(word)\n",
    "            else:\n",
    "                groups_of_subwords = self._split_word(word)\n",
    "                for group in groups_of_subwords:\n",
    "                    group[0] = group[0].replace('##','')\n",
    "                groups_of_valid_subwords = list(\n",
    "                        filter(\n",
    "                            lambda group: all(\n",
    "                                subword in freq_dict for subword in group\n",
    "                            ),\n",
    "                            groups_of_subwords,\n",
    "                        )\n",
    "                    )\n",
    "                if groups_of_valid_subwords:\n",
    "                    break\n",
    "        \n",
    "        if len(groups_of_valid_subwords) == 0:\n",
    "            output_tokens.append(self.unk_token)\n",
    "        else:\n",
    "            sorted_groups_of_valid_subwords = sorted(\n",
    "                groups_of_valid_subwords,\n",
    "                key=lambda group: sum(freq_dict[subword] for subword in group),\n",
    "            )\n",
    "            tokens = sorted_groups_of_valid_subwords[-1]\n",
    "            for token in tokens:\n",
    "                output_tokens.append(str(token))\n",
    "        return output_tokens\n",
    "    \n",
    "    def _split_word(self, word):\n",
    "        \"\"\"Split a word into a specific number of sub-words\n",
    "\n",
    "        Args:\n",
    "            word (str): word input\n",
    "            number_of_subwords (int): number of subtokens to generate from the word \n",
    "        \n",
    "        Returns:\n",
    "            list: list of subwords \n",
    "        \"\"\"\n",
    "        def _split(_word):\n",
    "            if not _word:\n",
    "                return\n",
    "            yield [f'##{_word}',]\n",
    "            for i in range(1, len(_word)):\n",
    "                for subwords in self._split_word(_word[i:]):\n",
    "                    yield [f'##{word[:i]}'] +subwords\n",
    "                    \n",
    "        subwords_groups = [group for group in _split(word)]\n",
    "        return subwords_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoTokenizer ...\n",
      "Training AutoTokenizer ...\n",
      "Training AutoTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "# training each tokenizer\n",
    "morph_generators = MorphGenerators()\n",
    "morph_generators.train()\n",
    "\n",
    "morph_bert = MorphBert()\n",
    "morph_bert.train()\n",
    "\n",
    "morph = tk.MorphologicalTokenizer()\n",
    "morph.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.4 µs ± 10 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "morph_generators.tokenize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.87 µs ± 344 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "morph_bert.tokenize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.93 µs ± 54.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "morph.tokenize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
