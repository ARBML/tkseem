{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import tkseem as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MorphologicalTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "origianl_morph = tk.MorphologicalTokenizer()\n",
    "origianl_morph.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModMorph(tk.MorphologicalTokenizer):\n",
    "    def _split_word(self, word):\n",
    "        \"\"\"Split a word into a specific number of sub-words\n",
    "\n",
    "        Args:\n",
    "            word (str): word input\n",
    "            number_of_subwords (int): number of subtokens to generate from the word \n",
    "        \n",
    "        Returns:\n",
    "            list: list of subwords \n",
    "        \"\"\"\n",
    "        def _split(_word):\n",
    "            if not _word:\n",
    "                return\n",
    "            yield [f'##{_word}',]\n",
    "            for i in range(1, len(_word)):\n",
    "                for subwords in self._split_word(_word[i:]):\n",
    "                    yield [f'##{word[:i]}'] +subwords\n",
    "        subwords = _split(word)\n",
    "        out_subwords = []\n",
    "        for group in subwords:\n",
    "            group[0] = group[0].replace('##','')\n",
    "            out_subwords.append(group)\n",
    "        return out_subwords\n",
    "\n",
    "        \n",
    "    def _tokenize_from_dict(self, text, freq_dict, cache=False, max_size=20):\n",
    "        \"\"\"Tokenize using frequency based approach given a dictionary\n",
    "\n",
    "        Args:\n",
    "            text (str): input string\n",
    "            freq_dict (dict): frequency dictionary\n",
    "            cache (bool, optional): faster approach. Defaults to False.\n",
    "            max_size (int, optional): maximum word size. Defaults to 20.\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "        \"\"\"\n",
    "        assert freq_dict\n",
    "        tokens = []\n",
    "        output_tokens = []\n",
    "        for word in text.split():\n",
    "            if len(word) >= max_size:\n",
    "                print(f\"{word} is too long ...\")\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "            if word in freq_dict:\n",
    "                output_tokens.append(word)\n",
    "            else:\n",
    "                groups_of_subwords = self._split_word(word)\n",
    "                groups_of_valid_subwords = list(\n",
    "                        filter(\n",
    "                            lambda group: all(\n",
    "                                subword in freq_dict for subword in group\n",
    "                            ),\n",
    "                            groups_of_subwords,\n",
    "                        )\n",
    "                    )\n",
    "                if groups_of_valid_subwords:\n",
    "                    break\n",
    "                if not next(groups_of_valid_subwords):\n",
    "                    output_tokens.append(self.unk_token)\n",
    "                else:\n",
    "                    print(list(groups_of_valid_subwords))\n",
    "                    sorted_groups_of_valid_subwords = sorted(\n",
    "                        groups_of_valid_subwords,\n",
    "                        key=lambda group: sum(freq_dict[subword] for subword in group),\n",
    "                    )\n",
    "                    tokens = sorted_groups_of_valid_subwords[-1]\n",
    "                    for token in tokens:\n",
    "                        output_tokens.append(str(token))\n",
    "        return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MorphologicalTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "mod_morph = ModMorph()\n",
    "mod_morph.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'السلام'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 380 µs, sys: 36 µs, total: 416 µs\n",
      "Wall time: 426 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "subs = list()\n",
    "for i in range(1,len(word)):\n",
    "    subs += origianl_morph._split_word(word,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 608 µs, sys: 59 µs, total: 667 µs\n",
      "Wall time: 684 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "mod_morph._split_word(word)\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
