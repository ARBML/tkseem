{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poetry Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tkseem\n",
    "!pip install tnkeeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/ARBML/tkseem/master/tasks/meter_classification/meters/data.txt\n",
    "!wget https://raw.githubusercontent.com/ARBML/tkseem/master/tasks/meter_classification/meters/labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tkseem as tk\n",
    "import tnkeeh as tn\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import GRU, Embedding, Dense, Input, Dropout, Bidirectional, BatchNormalization, Flatten, Reshape\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove diacritics\n",
      "Remove Tatweel\n",
      "Saving to meters/cleaned_data.txt\n",
      "Split data\n",
      "Save to data\n",
      "Read data  ['test_data.txt', 'test_lbls.txt', 'train_data.txt', 'train_lbls.txt']\n"
     ]
    }
   ],
   "source": [
    "tn.clean_data(file_path = 'meters/data.txt', save_path = 'meters/cleaned_data.txt', remove_diacritics=True, \n",
    "      execluded_chars=['!', '.', '?', '#'])\n",
    "tn.split_classification_data('meters/cleaned_data.txt', 'meters/labels.txt')\n",
    "train_data, test_data, train_lbls, test_lbls = tn.read_data(mode = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(data) for data in train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.CharacterTokenizer()\n",
    "tokenizer.train('data/train_data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokenizer, data, labels):\n",
    "    X = tokenizer.encode_sentences(data)\n",
    "    y = np.array([int(lbl) for lbl in labels])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process training data\n",
    "X_train, y_train = preprocess(tokenizer, train_data, train_lbls)\n",
    "\n",
    "# process test data\n",
    "X_test, y_test = preprocess(tokenizer, test_data, test_lbls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input((100,)))\n",
    "model.add(Embedding(tokenizer.vocab_size, 256))\n",
    "model.add(Bidirectional(GRU(units = 256, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(units = 256, return_sequences=True)))\n",
    "model.add(Bidirectional(GRU(units = 256)))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(14, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"input_1:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 86).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"input_1:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 86).\n",
      "133/133 [==============================] - ETA: 0s - loss: 2.2590 - accuracy: 0.1899WARNING:tensorflow:Model was constructed with shape (None, 100) for input Tensor(\"input_1:0\", shape=(None, 100), dtype=float32), but it was called on an input with incompatible shape (None, 86).\n",
      "133/133 [==============================] - 435s 3s/step - loss: 2.2590 - accuracy: 0.1899 - val_loss: 1.9318 - val_accuracy: 0.2881\n",
      "Epoch 2/10\n",
      "133/133 [==============================] - 399s 3s/step - loss: 1.7722 - accuracy: 0.3391 - val_loss: 1.6506 - val_accuracy: 0.3907\n",
      "Epoch 3/10\n",
      "133/133 [==============================] - 383s 3s/step - loss: 1.5203 - accuracy: 0.4517 - val_loss: 1.2544 - val_accuracy: 0.5645\n",
      "Epoch 4/10\n",
      "133/133 [==============================] - 385s 3s/step - loss: 1.0520 - accuracy: 0.6415 - val_loss: 0.7986 - val_accuracy: 0.7440\n",
      "Epoch 5/10\n",
      "133/133 [==============================] - 382s 3s/step - loss: 0.7140 - accuracy: 0.7734 - val_loss: 0.6061 - val_accuracy: 0.7997\n",
      "Epoch 6/10\n",
      "133/133 [==============================] - 381s 3s/step - loss: 0.5413 - accuracy: 0.8353 - val_loss: 0.5232 - val_accuracy: 0.8395\n",
      "Epoch 7/10\n",
      "133/133 [==============================] - 386s 3s/step - loss: 0.4214 - accuracy: 0.8757 - val_loss: 0.4718 - val_accuracy: 0.8589\n",
      "Epoch 8/10\n",
      "110/133 [=======================>......] - ETA: 1:05 - loss: 0.3336 - accuracy: 0.9029"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_split = 0.1, epochs = 10, batch_size= 256, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2name = ['السريع', 'الكامل', 'المتقارب', 'المتدارك', 'المنسرح', 'المديد', \n",
    "              'المجتث', 'الرمل', 'البسيط', 'الخفيف', 'الطويل', 'الوافر', 'الهزج', 'الرجز']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(sentence):\n",
    "    sequence = tokenizer.encode_sentences([sentence], out_length = max_length)\n",
    "    pred = model.predict(sequence)[0]\n",
    "    print(label2name[np.argmax(pred, 0).astype('int')], np.max(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الرمل 0.99949336\n",
      "الكامل 0.9249198\n",
      "الكامل 0.9218475\n",
      "الطويل 0.9974923\n",
      "المجتث 0.6107578\n",
      "المديد 0.9062244\n",
      "الهزج 0.9060462\n",
      "السريع 0.832686\n",
      "المتدارك 0.6292933\n",
      "المتدارك 0.60360384\n"
     ]
    }
   ],
   "source": [
    "classify(\"ما تردون على هذا المحب # دائبا يشكو إليكم في الكتب\")\n",
    "classify(\"ولد الهدى فالكائنات ضياء # وفم الزمان تبسم وسناء\")\n",
    "classify(\" لك يا منازل في القلوب منازل # أقفرت أنت وهن منك أواهل\")\n",
    "classify(\"ومن لم يمت بالسيف مات بغيره # تعددت الأسباب والموت واحد\")\n",
    "classify(\"أنا النبي لا كذب # أنا ابن عبد المطلب\")\n",
    "classify(\"هذه دراهم اقفرت # أم ربور محتها الدهور\")\n",
    "classify(\"هزجنا في بواديكم # فأجزلتم عطايانا\")\n",
    "classify(\"بحر سريع ماله ساحل # مستفعلن مستفعلن فاعلن\")\n",
    "classify(\"مَا مَضَى فَاتَ وَالْمُؤَمَّلُ غَيْبٌ # وَلَكَ السَّاعَةُ الَّتِيْ أَنْتَ فِيْهَا\")\n",
    "classify(\"يا ليلُ الصبّ متى غدهُ # أقيامُ الساعة موعدهُ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
