{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tkseem in /home/zaid/.local/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: numpy in /home/zaid/.local/lib/python3.8/site-packages (from tkseem) (1.19.0)\n",
      "Requirement already satisfied: black in /home/zaid/.local/lib/python3.8/site-packages (from tkseem) (19.10b0)\n",
      "Requirement already satisfied: tqdm in /home/zaid/.local/lib/python3.8/site-packages (from tkseem) (4.46.1)\n",
      "Requirement already satisfied: farasapy in /home/zaid/.local/lib/python3.8/site-packages (from tkseem) (0.0.4)\n",
      "Requirement already satisfied: sentencepiece in /home/zaid/.local/lib/python3.8/site-packages (from tkseem) (0.1.91)\n",
      "Requirement already satisfied: toml>=0.9.4 in /home/zaid/.local/lib/python3.8/site-packages (from black->tkseem) (0.10.1)\n",
      "Requirement already satisfied: regex in /home/zaid/.local/lib/python3.8/site-packages (from black->tkseem) (2020.7.14)\n",
      "Requirement already satisfied: pathspec<1,>=0.6 in /home/zaid/.local/lib/python3.8/site-packages (from black->tkseem) (0.8.0)\n",
      "Requirement already satisfied: typed-ast>=1.4.0 in /home/zaid/.local/lib/python3.8/site-packages (from black->tkseem) (1.4.1)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/lib/python3/dist-packages (from black->tkseem) (19.3.0)\n",
      "Requirement already satisfied: appdirs in /home/zaid/.local/lib/python3.8/site-packages (from black->tkseem) (1.4.4)\n",
      "Requirement already satisfied: click>=6.5 in /usr/lib/python3/dist-packages (from black->tkseem) (7.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from farasapy->tkseem) (2.22.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tkseem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkseem as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.train('samples/data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[557, 798]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"السلام عليكم\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['السلام', 'عليكم']\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "السلام عليكم\n"
     ]
    }
   ],
   "source": [
    "detokenized = tokenizer.detokenize(decoded)\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SentencePiece ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.SentencePieceTokenizer()\n",
    "tokenizer.train('samples/data.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1799, 2741]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"السلام عليكم\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁السلام', '▁عليكم']\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " السلام عليكم\n"
     ]
    }
   ],
   "source": [
    "detokenized = tokenizer.detokenize(decoded)\n",
    "print(detokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AutoTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.MorphologicalTokenizer()\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 367, 764, 184]\n"
     ]
    }
   ],
   "source": [
    "encoded = tokenizer.encode(\"السلام عليكم\")\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ال', '##سلام', 'علي', '##كم']\n"
     ]
    }
   ],
   "source": [
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RandomTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.RandomTokenizer()\n",
    "tokenizer.train('samples/data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ا', '##لسلام', 'علي', '##كم', 'أ', '##يها', 'الأ', '##صدقاء']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disjoint Letter Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DisjointLetterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.DisjointLetterTokenizer()\n",
    "tokenizer.train('samples/data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ا', '##لسلا', '##م', 'عليكم', 'أ', '##يها', 'ا', '##لأ', '##صد', '##قا', '##ء']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.CharacterTokenizer()\n",
    "tokenizer.train('samples/data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ا', '##ل', '##س', '##ل', '##ا', '##م', 'ع', '##ل', '##ي', '##ك', '##م']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved for deployment and reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n",
      "Saving as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.train('samples/data.txt')\n",
    "tokenizer.save_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model without pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.load_model('freq.pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('السلام عليكم')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training WordTokenizer ...\n",
      "Training SentencePiece ...\n",
      "Training RandomTokenizer ...\n",
      "Training DisjointLetterTokenizer ...\n",
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import time \n",
    "\n",
    "def calc_time(fun):\n",
    "    start_time = time.time()\n",
    "    fun().train('samples/data.txt')\n",
    "    return time.time() - start_time\n",
    "\n",
    "running_times = {}\n",
    "\n",
    "running_times['Word'] = calc_time(tk.WordTokenizer)\n",
    "running_times['SP'] = calc_time(tk.SentencePieceTokenizer)\n",
    "running_times['Random'] = calc_time(tk.RandomTokenizer)\n",
    "running_times['Disjoint'] = calc_time(tk.DisjointLetterTokenizer)\n",
    "running_times['Character'] = calc_time(tk.CharacterTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f024ce83a90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(data = pd.DataFrame.from_dict([running_times]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
