{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training FrequencyTokenizer...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[536, 829]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([536, 829])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training SentencePiece...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.SentencePieceTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3777, 1424, 78, 423, 9962, 560]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3777, 1424, 78, 423, 9962, 560])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training AutoTokenizer...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.AutoTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 367, 764, 184]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2, 367, 764, 184])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training RandomTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.RandomTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ا', '##لسلام', 'علي', '##كم', 'أي', '##ها', 'الأص', '##دقاء']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disjoint Letter Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training DisjointLetterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.DisjointLetterTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السل', '##ام', 'عليكم', 'أيه', '##ا', 'ال', '##أص', '##دق', '##ا', '##ء']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use memory mapping to extract token's frequency for large files. It uses `mmap` to process chunks of the data at each iteration step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training FrequencyTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "\n",
    "# initialize\n",
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "\n",
    "# training \n",
    "tokenizer.train(large_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization vs Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tokenization to segment words using a pretrained dictionary. This makes segmentation very fast as compared to\n",
    "using libraries like `farasa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training AutoTokenizer...\n",
      "25.84449815750122\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "import time\n",
    "\n",
    "tokenizer = tk.AutoTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()\n",
    "\n",
    "start_time = time.time()\n",
    "out = tokenizer.tokenize(open('data/raw/train.txt').read(), cache = True)\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Farasa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaid/.local/lib/python3.8/site-packages/farasa/__base.py:43: UserWarning: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Segmenting the data ...\n",
      "Splitting the data ...\n",
      "47.383551836013794\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer(segment = True)\n",
    "start_time = time.time()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved for deployment and reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training FrequencyTokenizer...\n",
      "Saving as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()\n",
    "tokenizer.save_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model without pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Loading as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.load_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FrequencyTokenizer...\n",
      "Training SentencePiece...\n",
      "Training RandomTokenizer ...\n",
      "Training AutoTokenizer...\n",
      "Training DisjointLetterTokenizer ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f67e4f3efd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "import time \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def calc_time(fun):\n",
    "    start_time = time.time()\n",
    "    fun().train()\n",
    "    return time.time() - start_time\n",
    "\n",
    "running_times = {}\n",
    "\n",
    "running_times['Word'] = calc_time(tk.WordTokenizer)\n",
    "running_times['SentencePiece'] = calc_time(tk.SentencePieceTokenizer)\n",
    "running_times['Random'] = calc_time(tk.RandomTokenizer)\n",
    "running_times['Auto'] = calc_time(tk.AutoTokenizer)\n",
    "running_times['Disjoint'] = calc_time(tk.DisjointLetterTokenizer)\n",
    "sns.barplot(data = pd.DataFrame.from_dict([running_times]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
