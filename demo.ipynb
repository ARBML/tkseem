{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers as tk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training FrequencyTokenizer...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[536, 829]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السلام', 'عليكم']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([536, 829])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training SentencePiece...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.SentencePieceTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3777, 1424, 78, 423, 9962, 560]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"صباح الخير يا أصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁صباح', '▁الخير', '▁يا', '▁أص', 'د', 'قاء']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3777, 1424, 78, 423, 9962, 560])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read, preprocess then train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training AutoTokenizer...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.AutoTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode as ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 367, 764, 184]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decode back to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ال', '##سلام', 'علي', '##كم']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([2, 367, 764, 184])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training RandomTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.RandomTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ا', '##لسلام', 'علي', '##كم', 'أي', '##ها', 'الأص', '##دقاء']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disjoint Letter Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training DisjointLetterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.DisjointLetterTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['السل', '##ام', 'عليكم', 'أيه', '##ا', 'ال', '##أص', '##دق', '##ا', '##ء']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم أيها الأصدقاء\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training CharacterTokenizer ...\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "tokenizer = tk.CharacterTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ا', '##ل', '##س', '##ل', '##ا', '##م', 'ع', '##ل', '##ي', '##ك', '##م']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"السلام عليكم\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use memory mapping to extract token's frequency for large files. It uses `mmap` to process chunks of the data at each iteration step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training FrequencyTokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  5.40it/s]\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "\n",
    "# initialize\n",
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "\n",
    "# training \n",
    "tokenizer.train(large_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization vs Segmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tokenization to segment words using a pretrained dictionary. This makes segmentation very fast as compared to\n",
    "using libraries like `farasa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training AutoTokenizer...\n",
      "25.84449815750122\n"
     ]
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "import time\n",
    "\n",
    "tokenizer = tk.AutoTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()\n",
    "\n",
    "start_time = time.time()\n",
    "out = tokenizer.tokenize(open('data/raw/train.txt').read(), cache = True)\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Farasa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaid/.local/lib/python3.8/site-packages/farasa/__base.py:43: UserWarning: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Segmenting the data ...\n",
      "Splitting the data ...\n",
      "47.383551836013794\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer(segment = True)\n",
    "start_time = time.time()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "end_time = time.time()\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved for deployment and reloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Training FrequencyTokenizer...\n",
      "Saving as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.train()\n",
    "tokenizer.save_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model without pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading the data ...\n",
      "Splitting the data ...\n",
      "Loading as pickle file ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tk.WordTokenizer()\n",
    "tokenizer.process_data('samples/data.txt')\n",
    "tokenizer.load_model('freq.pl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FrequencyTokenizer...\n",
      "Training SentencePiece...\n",
      "Training RandomTokenizer ...\n",
      "Training AutoTokenizer...\n",
      "Training DisjointLetterTokenizer ...\n",
      "Training CharacterTokenizer ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f74eefd5880>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tokenizers as tk\n",
    "import time \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def calc_time(fun):\n",
    "    start_time = time.time()\n",
    "    fun().train()\n",
    "    return time.time() - start_time\n",
    "\n",
    "running_times = {}\n",
    "\n",
    "running_times['Word'] = calc_time(tk.WordTokenizer)\n",
    "running_times['SP'] = calc_time(tk.SentencePieceTokenizer)\n",
    "running_times['Random'] = calc_time(tk.RandomTokenizer)\n",
    "running_times['Auto'] = calc_time(tk.AutoTokenizer)\n",
    "running_times['Disjoint'] = calc_time(tk.DisjointLetterTokenizer)\n",
    "running_times['Character'] = calc_time(tk.CharacterTokenizer)\n",
    "sns.barplot(data = pd.DataFrame.from_dict([running_times]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
